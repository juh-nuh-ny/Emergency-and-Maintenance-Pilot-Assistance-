{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jananinareshkumar/miniforge3/envs/TF_oneLastTime/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/jananinareshkumar/miniforge3/envs/TF_oneLastTime/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Text extraction from each pdf\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# 2. Load and extract all pdfs from a directory\n",
    "def load_all_pdfs(pdf_directory):\n",
    "    documents=[]\n",
    "    \n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            full_path = os.path.join(pdf_directory, filename)\n",
    "            print(f\"Processing {full_path}...\")\n",
    "            text = extract_text_from_pdf(full_path)\n",
    "            documents.append(text)\n",
    "            \n",
    "    return documents\n",
    "\n",
    "# 3. Chunk into retrievable sections\n",
    "def chunk_documents(documents, chunk_size=500, chunk_overlap=50):\n",
    "    #tunable but works fine eitherways\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks=[]\n",
    "    for doc in documents:\n",
    "        doc_chunks = text_splitter.split_text(doc)\n",
    "        chunks.extend(doc_chunks)\n",
    "    return chunks\n",
    "\n",
    "# 4. Chunk Caching\n",
    "def load_or_generate_chunks(pdf_directory, chunks_cache_file=\"chunks_cache.pkl\", chunk_size=500, chunk_overlap=50):\n",
    "    if os.path.exists(chunks_cache_file):\n",
    "        print(f\"Loading cached chunks from {chunks_cache_file}...\")\n",
    "        with open(chunks_cache_file, \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached chunks found. Processing PDFs to generate chunks...\")\n",
    "        documents = load_all_pdfs(pdf_directory)\n",
    "        chunks = chunk_documents(documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        print(f\"Caching {len(chunks)} chunks to {chunks_cache_file}...\")\n",
    "        with open(chunks_cache_file, \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "    return chunks\n",
    "\n",
    "# 5. Embeddings generation\n",
    "def generate_embeddings(text_list, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32):\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    embeddings = model.encode(text_list, batch_size=batch_size, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "# 6. Loading cache embeddings for saving time\n",
    "def load_or_generate_embeddings(chunks, cache_file=\"embeddings_cache.pkl\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32):\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            embeddings = pickle.load(f)\n",
    "            \n",
    "        if embeddings.shape[0] != len(chunks):\n",
    "            print(\"Mismatch between cached embeddings and current chunk count. Regenerating embeddings.\")\n",
    "            embeddings = generate_embeddings(chunks, model_name=model_name, batch_size = batch_size)\n",
    "            with open (cache_file, \"wb\") as f:\n",
    "                pickle.dump(embeddings, f)\n",
    "    else:\n",
    "        print(\"No cached embeddings found, generating new ones...\")\n",
    "        embeddings = generate_embeddings(chunks, model_name = model_name, batch_size = batch_size)\n",
    "        print(f\"Caching embeddings to {cache_file}\")\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "    \n",
    "    return embeddings\n",
    "        \n",
    "# 7. FAISS index build for similarity search\n",
    "def build_faiss_index(embeddings):\n",
    "    if torch.is_tensor(embeddings):\n",
    "        embeddings_np = embeddings.cpu().numpy()\n",
    "    else:\n",
    "        embeddings_np = np.array(embeddings)\n",
    "        \n",
    "    dim = embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings_np)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading or generating chunks\n",
      "Loading cached chunks from chunks_cache.pkl...\n",
      "13383 chunks loaded.\n",
      "Embedding Generation\n",
      "FAISS Index Setup\n",
      "FAISS Index contains 13383 vectors.\n",
      "Using device: mps\n",
      "Top 5 matches [[4414 3635 3629  252 8764]]\n",
      "\n",
      "Chunk index 4414 with L2 distance 1.0111110210418701:\n",
      "Hydraulics\n",
      "737 Flight Crew Operations Manual\n",
      "Table of Contents\n",
      "Intentionally\n",
      "Blank\n",
      "13.TOC.2 Copyright ©The Boeing Company. See title page for details.\n",
      "D6-27370-804-BRI(P2) July 17, 2009\n",
      "13.1\n",
      "737 Flight Crew Operations Manual\n",
      "NLONWC. 1P3R NEoSnS-UNRoErm -a Hl Cydhreacuklliics tPs-uHmypd (rsaiunlgicles)\n",
      "HYDRAULIC PUMP LOW\n",
      "LOW\n",
      "PRESSURE PRESSURE\n",
      "Condition: The hydraulic pump pressure is low.\n",
      "1 HYD PUMP switch (affected side) . . . . . . . . . OFF\n",
      "Note: Loss of an engine-driven hydraulic pump and\n",
      "================================================================================\n",
      "\n",
      "Chunk index 3635 with L2 distance 1.056025743484497:\n",
      " When CAB PR ΔP < 1 psi:\n",
      "RAM AIR........................................................................................ON\n",
      "FOR TRAINING PURPOSES ONLY\n",
      "ABNORMAL AND 11.01D\n",
      "EMERGENCY PROCEDURES\n",
      "A318/A319/A320/A321 23 NOV 21\n",
      "QUICK REFERENCE HANDBOOK\n",
      "BLEED 1+2 FAULT (Cont'd)\n",
      " If both LEFT LEAK and RIGHT LEAK subtitles with AIR ENG 1+2 BLEED\n",
      "FAULT alert\n",
      "or\n",
      "If both engine bleeds lost due to engine fire or Start Air Valve failed open or\n",
      "APU leak fed by engine:\n",
      "NO ENGINE BLEED CAN BE RECOVERED\n",
      "================================================================================\n",
      "\n",
      "Chunk index 3629 with L2 distance 1.1086735725402832:\n",
      " If engine 2 bleed not recovered:\n",
      "ENG 2 BLEED.......................................................................................OFF\n",
      "MAX FL: 100 / MEA-MORA\n",
      "WING A.ICE NOT AVAILABLE\n",
      " When CAB PR ΔP < 1 psi:\n",
      "RAM AIR.............................................................................................ON\n",
      " If RIGHT LEAK subtitle with AIR ENG 1+2 BLEED FAULT alert\n",
      "or\n",
      "If engine 2 bleed lost due to engine 2 fire or Start Air Valve 2 failed open:\n",
      "================================================================================\n",
      "\n",
      "Chunk index 252 with L2 distance 1.138809084892273:\n",
      "(ii) In-flight failure of hydraulic systems that results in sustained reliance on the sole remaining\n",
      "hydraulic or mechanical system for movement of flight control surfaces;\n",
      "(iii) Sustained loss of the power or thrust produced by two or more engines; and\n",
      "(iv) An evacuation of an aircraft in which an emergency egress system is utilized.\n",
      "(8) Release of all or a portion of a propeller blade from an aircraft, excluding release caused solely by\n",
      "ground contact;\n",
      "================================================================================\n",
      "\n",
      "Chunk index 8764 with L2 distance 1.1394792795181274:\n",
      "engine” is used to assist in identifying the failed engine. Depending on the failure mode, the pilot will not be able to consistently\n",
      "identify the failed engine in a timely manner from the engine gauges. In maintaining directional control, however, rudder pressure is\n",
      "exerted on the side (left or right) of the airplane with the operating engine. Thus, the “dead foot” is on the same side as the “dead\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pdf_directory = \"/Users/jananinareshkumar/Desktop/rag/fly\"\n",
    "    \n",
    "    # Load or generate chunks from the PDFs (using cached chunks if available)\n",
    "    print(\"Loading or generating chunks\")\n",
    "    chunks = load_or_generate_chunks(pdf_directory, chunks_cache_file=\"chunks_cache.pkl\", chunk_size=500, chunk_overlap=50)\n",
    "    print(f\"{len(chunks)} chunks loaded.\")\n",
    "    \n",
    "    # Generate or load cached embeddings for these chunks\n",
    "    print(\"Embedding Generation\")\n",
    "    embeddings = load_or_generate_embeddings(chunks, cache_file=\"embeddings_cache.pkl\")\n",
    "    \n",
    "    # Build the FAISS index\n",
    "    print(\"FAISS Index Setup\")\n",
    "    index = build_faiss_index(embeddings)\n",
    "    print(f\"FAISS Index contains {index.ntotal} vectors.\")\n",
    "    \n",
    "    # Query the index with a sample query\n",
    "    query_text = \"hydraulic failure in left engine\"\n",
    "    query_embedding = generate_embeddings([query_text])\n",
    "    \n",
    "    # Convert the query embedding to a NumPy array for FAISS\n",
    "    if torch.is_tensor(query_embedding):\n",
    "        query_np = query_embedding.cpu().numpy()\n",
    "    else:\n",
    "        query_np = np.array(query_embedding)\n",
    "    \n",
    "    k = 5  # Number of nearest neighbors to retrieve\n",
    "    distances, indices = index.search(query_np, k)\n",
    "    print(\"Top 5 matches\", indices)\n",
    "    \n",
    "    # Optionally, print the text for each retrieved chunk\n",
    "    for distance, idx in zip(distances[0], indices[0]):\n",
    "        print(f\"\\nChunk index {idx} with L2 distance {distance}:\")\n",
    "        print(chunks[idx])\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_oneLastTime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
